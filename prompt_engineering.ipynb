{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Open LLMs\n",
    "This project explores different prompt engineering techniques using an **open-source LLM** via Hugging Face API.\n",
    "\n",
    "## Objectives\n",
    "We will implement:\n",
    "- **Zero-shot prompting** (No examples provided).\n",
    "- **Few-shot prompting** (Providing examples for better responses).\n",
    "- **Chain-of-thought reasoning** (Step-by-step explanations for improved answers).\n",
    "\n",
    "We will use an **Open LLM from Hugging Face** to test how different prompt styles affect responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Api key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded succesfully\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "API_KEY = config.HUGGINGFACE_API_KEY\n",
    "MODEL_NAME = config.MODEL_NAME\n",
    "\n",
    "print(\"API key loaded succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import config\n",
    "\n",
    "importlib.reload(config)  # Force reload the config file\n",
    "\n",
    "# Now, reassign values\n",
    "API_KEY = config.HUGGINGFACE_API_KEY\n",
    "MODEL_NAME = config.MODEL_NAME\n",
    "\n",
    "print(\"üîç Debug: API URL -\", f\"https://api-inference.huggingface.co/models/{MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key Setup\n",
    "To interact with the Hugging Face API, we need an **API key**.\n",
    "- The key is securely stored in `config.py` to avoid exposing it in the notebook.\n",
    "- We load the key in the notebook using `import config`.\n",
    "- The model we are using is **Mistral-7B-Instruct**, but this can be changed.\n",
    "\n",
    "### ‚úÖ Next Step: Writing the API Request Function\n",
    "Now that our API key is ready, we will write a function to send a request to the Hugging Face model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_hugging_face(prompt, max_length = 200, temperature = 0.7):\n",
    "    \"\"\"\n",
    "    Sends a request to the Hugging Face API and returns the generated response.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input text for the model.\n",
    "    - max_length (int): Maximum number of tokens in the response.\n",
    "    - temperature (float): Controls randomness (higher = more creative).\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's response.\n",
    "    \"\"\"\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_NAME}\"\n",
    "    print(f\"üîç Debug: API URL - {API_URL}\") \n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_length,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload, timeout=10)  # 10s timeout\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "\n",
    "        response_json = response.json()  # Convert response to JSON\n",
    "        generated_text = response_json[0].get('generated_text', \"No response generated.\")\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        generated_text = \"Error: The server took too long to respond. Try again later.\"\n",
    "    \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        generated_text = \"Error: Could not connect to Hugging Face API. Check your internet or try later.\"\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        generated_text = f\"HTTP Error: {http_err}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        generated_text = f\"Error: {str(e)}\"\n",
    "    return generated_text  # Return the final text output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n",
      "\n",
      "üîπ Generated Response:\n",
      "\n",
      "What are some interesting facts about space? There are many interesting facts\n",
      "about space! Some of the most fascinating include that the universe is believed\n",
      "to be around 13.8 billion years old, that there is evidence of water on the\n",
      "moon, that there are black holes in the universe, and that the first satellite,\n",
      "Sputnik 1, was launched into orbit in 1957. Here are just a few more examples:\n",
      "- The largest asteroid in the solar system is called Ceres, and it's over 260\n",
      "miles wide! - Saturn's moon, Enceladus, has an underground ocean of liquid\n",
      "water. - The universe contains far more dark matter than light matter, and\n",
      "scientists are still working to understand its properties. - The most distant\n",
      "galaxies have been discovered, and they're thought to be over 13.8 billion years\n",
      "old! - The first satellite, Sputnik 1, was launched into orbit in 1957. - There\n",
      "are over 1\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "test_prompt = \"What are some interesting facts about space?\"\n",
    "response = query_hugging_face(test_prompt)\n",
    "\n",
    "# Wrap text to 80 characters per line for better readability\n",
    "wrapped_response = \"\\n\".join(textwrap.wrap(response, width=80))\n",
    "\n",
    "print(\"\\nüîπ Generated Response:\\n\")\n",
    "print(wrapped_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n",
      "\n",
      "üîπ Generated Response:\n",
      "\n",
      "explain zero-shot prompting in LLMs? Zero-shot prompting in LLMs is used to\n",
      "train a language model on a dataset that does not contain any specific labels or\n",
      "categories. Instead, the model is asked to predict the next word or sentence\n",
      "that should come after the input text. This is challenging for a language model\n",
      "because it requires the model to understand the context of the text without any\n",
      "prior knowledge of the content. This type of prompting can be effective in tasks\n",
      "such as text classification and translation, where the goal is to understand the\n",
      "meaning of a sentence or text in a given language.\n"
     ]
    }
   ],
   "source": [
    "test_p2 = \"explain zero-shot prompting in LLMs?\"\n",
    "response2 = query_hugging_face(test_p2)\n",
    "\n",
    "# Wrap text to 80 characters per line for better readability\n",
    "wrapped_response2 = \"\\n\".join(textwrap.wrap(response2, width=80))\n",
    "\n",
    "print(\"\\nüîπ Generated Response:\\n\")\n",
    "print(wrapped_response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n",
      "\n",
      "üîπ Improved Response:\n",
      "\n",
      "In simple terms, what is zero-shot prompting in LLMs? Explain without technical\n",
      "jargon. Zero-shot prompting is a technique in natural language processing where\n",
      "the system tries to predict the next word or phrase based on the words that have\n",
      "been seen so far in a conversation or text, without being given any prior\n",
      "context about those words.\n"
     ]
    }
   ],
   "source": [
    "test_p3 = \"In simple terms, what is zero-shot prompting in LLMs? Explain without technical jargon.\"\n",
    "response3 = query_hugging_face(test_p3)\n",
    "\n",
    "wrapped_response3 = \"\\n\".join(textwrap.wrap(response3, width=80))\n",
    "\n",
    "print(\"\\nüîπ Improved Response:\\n\")\n",
    "print(wrapped_response3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n",
      "\n",
      "üîπ Final Refined Response:\n",
      "\n",
      "Explain zero-shot prompting in LLMs. Focus on how it works when a user asks a\n",
      "question without providing examples. Zero-shot prompting is a technique in\n",
      "natural language processing (NLP) that allows a machine learning model (such as\n",
      "a neural network) to learn from a user's question without being explicitly told\n",
      "what the user wants to know. The model uses the question as a prompt to generate\n",
      "a response, which it then evaluates for accuracy. In essence, the model learns\n",
      "to generate a response based on a single example, rather than being trained on a\n",
      "list of examples with associated labels.\n"
     ]
    }
   ],
   "source": [
    "test_p4 = \"Explain zero-shot prompting in LLMs. Focus on how it works when a user asks a question without providing examples.\"\n",
    "response4 = query_hugging_face(test_p4)\n",
    "\n",
    "wrapped_response4 = \"\\n\".join(textwrap.wrap(response4, width=80))\n",
    "\n",
    "print(\"\\nüîπ Final Refined Response:\\n\")\n",
    "print(wrapped_response4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n",
      "\n",
      "üîπ Few-Shot Response:\n",
      "\n",
      "Classify the sentiment of the following sentences as Positive or Negative:\n",
      "Sentence: \"I love this movie!\" Sentiment: Positive  Sentence: \"This is the worst\n",
      "experience ever.\" Sentiment: Negative  Sentence: \"The food was delicious and the\n",
      "service was great.\" Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "test_few_shot = \"\"\"Classify the sentiment of the following sentences as Positive or Negative:\n",
    "\n",
    "Sentence: \"I love this movie!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Sentence: \"This is the worst experience ever.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Sentence: \"The food was delicious and the service was great.\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "response_few_shot = query_hugging_face(test_few_shot)\n",
    "\n",
    "wrapped_response_few_shot = \"\\n\".join(textwrap.wrap(response_few_shot, width=80))\n",
    "\n",
    "print(\"\\nüîπ Few-Shot Response:\\n\")\n",
    "print(wrapped_response_few_shot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug: API URL - https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\n",
      "\n",
      "üîπ Chain-of-Thought Response:\n",
      "\n",
      "I have to move out in 30 minutes to catch a cricket practice session.  Before\n",
      "leaving, I need to complete the following tasks: - Take a leak (5 to 7 minutes)\n",
      "- Freshen up (5 to 7 minutes) - Get clothes from the closet (5 to 7 minutes) -\n",
      "Get ready (5 to 7 minutes)  Each task takes between **5 to 7 minutes**.   Step-\n",
      "by-step, calculate: 1. The **average time per task** (midpoint of 5 to 7). 2.\n",
      "The **total time needed** for all tasks. 3. The **remaining buffer time** before\n",
      "leaving.  Show your work step by step before giving the final answer.  <p>Task\n",
      "1: 5 minutes (average time per task). Task 2: 7 minutes (average time per task).\n",
      "Task 3: 5 minutes (average time per task). Remaining buffer time: 5 + 5 + 7 = 17\n",
      "minutes.</p>  <p>Therefore, the total time needed will be:</p>  <p>Remaining\n",
      "buffer time (17 minutes) + Freshen up (7 minutes) + Take a leak (5 minutes) +\n",
      "Get ready (5 minutes) = 23 minutes.</p>  <p>So, there is a buffer time of 17\n",
      "minutes, which means there is enough time to complete all three tasks.\n",
      "Therefore, the answer is:</p>  <p>Remaining buffer time (17 minutes) + the time\n",
      "to complete the following tasks in 30 minutes = 5 minutes\n"
     ]
    }
   ],
   "source": [
    "test_CoT = \"\"\"I have to move out in 30 minutes to catch a cricket practice session. \n",
    "Before leaving, I need to complete the following tasks:\n",
    "- Take a leak (5 to 7 minutes)\n",
    "- Freshen up (5 to 7 minutes)\n",
    "- Get clothes from the closet (5 to 7 minutes)\n",
    "- Get ready (5 to 7 minutes)\n",
    "\n",
    "Each task takes between **5 to 7 minutes**. \n",
    "\n",
    "Step-by-step, calculate:\n",
    "1. The **average time per task** (midpoint of 5 to 7).\n",
    "2. The **total time needed** for all tasks.\n",
    "3. The **remaining buffer time** before leaving.\n",
    "\n",
    "Show your work step by step before giving the final answer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response_CoT = query_hugging_face(test_CoT)\n",
    "\n",
    "wrapped_response_CoT = \"\\n\".join(textwrap.wrap(response_CoT, width=80))\n",
    "\n",
    "print(\"\\nüîπ Chain-of-Thought Response:\\n\")\n",
    "print(wrapped_response_CoT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
